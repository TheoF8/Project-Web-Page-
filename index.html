<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Communications Semester Project</title>
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <header>
    <h1>Using a 4M model to detect Alzheimer's Disease. </h1>
  </header>

  <main>
    <article class="blog-post">
      <h2>Project Description</h2>
      <p class="date">May 29, 2025</p>
      <p> 
        As life expectancy increases further into old age, neuro-degenerative diseases such as Alzheimer’s are becoming a major health concern in the modern 
         world. Early detection is crucial but interpreting complex brain scans is a demanding task. In our semester project, we investigated how 
        deep learning, specifically a robust encoder-only Transformer, can help classify MRI and PET scans to detect Alzheimer's.
        Using the ADNI dataset, we trained our model to recognize the subtle patterns in brain images that correlate with the disease. Our approach combines 
        representation learning and image compression to extract key features from medical scans — and then uses them to power an Alzheimer’s classifier.
        This blog showcases our methodology, results, and key insights. We hope it demonstrates the potential of machine learning in advancing neuroimaging 
        and medical diagnosis. <br>
        Below, we will present our findings regarding the use of an encoder-only Transformer inspired by Vision Transformers (ViT) in diagnosing Alzheimer disease.
      </p>
    </article>
    
    <article class="blog-post">
      <h2> Methodology and Data Used</h2>
      <p>
        We decided to use the ADNI database to train our model upon. The ADNI (Alzheimer's Disease Neuroimaging Initiative) database contains a plentitude of
        neurological brain scans from which we formed a cohort of specically MRI and PET scans tailored to our needs. <br>
        Before passing the imagery to our model, we have preprocessed it. Specifically, we have converted the dicom files into numpy array, normalized them and 
        rendered them more accessible and uniform for processing. We also removed all non-tissue parts from the scans (e.g. skull, etc..) and prioritized mid-brain
        regions abundant with grey matter. 
        <br>
        Here is a brief overview of how our encoder-only Transformer is operating. <br>
        First of all, all input images, genetic data, and clinical scores are tokenized and passed through a learnable embedding layer. To help the model distinguish 
        between input types, we also added modality-specific embeddings, because MRI tokens can carry different identity information than PET or APOE tokens. <br>
        Our core architecture consists of stacked self-attention blocks, each containing the following: a multi-head attention layer, feed-forward layers, residual 
        connections and a normalization layer. We allow flexibility in configuration (depth, attention head size, MLP ratio) so the model can be tuned for performance 
        vs. computational cost. <br>
        Finally, after being normalized and passing through two GELU layers, the pooled feature vector is converted into a category (chosen between normal, MCI and AD)
        by a softmax function. 
      </p>
    </article>
    
    <article class="blog-post">
      <h2> Results  </h2>
      <p>
        Our model has achieved the following results: (link to graphs) 
      </p>
    </article>

    <article class="blog-post">
      <h2> Encountered Challenges </h2>
      <p>
        We faced many challenges during the execution of this project. Namely, as described before, having chosen a specific cohort meant that we had not a lot
        of data samples (couple hundred) to work with. This means that our model wasn't trained to the extend that we wanted to. 
        <br> Another major issue was that 2D tokens do not include enough spatial information which is crucial for correct sample classification. We think that 
        using 3D customized VQ-VAE tokens would have greatly improved our results. 
        <br> Finally, the interpretability of transformer models remained unmastered, and the lack of diversity of the used dataset possibly prevented the model 
        from learning robust, generalizable patterns. 
        
        
      </p>
    </article>
    <article class="blog-post">
      <h2> Conclusion </h2>
      <p>
        In Conclusion, our modified Transformer encoder marks a shift from traditional convolutional approaches toward more flexible, multi-model architectures for 
        medical diagnosis. 
        By unifying MRI, PET, genetic, and clinical data within a single attention-based framework, the model is able to learn richer, modality-aware representations 
        that reflect the multifaceted nature of Alzheimer’s disease. While this approach is still in early stages, it demonstrates strong potential for handling complex 
        biomedical inputs and opens the door to more interpretable, generalizable, and multi-modal diagnostic models in the future.
      </p>
    </article>
     
  </main>
  <footer>
    <p>&copy; 2025 semester project <br>
      by Heeyoung Lee, Finn Mac Namara and Fedor Mitirev
    </p>
  </footer>
</body>
</html>
