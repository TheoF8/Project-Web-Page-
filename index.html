<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Communications Semester Project</title>
  <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap">
  <link rel="stylesheet" href="style.css">
  <script src="https://kit.fontawesome.com/a076d05399.js" crossorigin="anonymous"></script>
</head>
<body>
  <header>
    <h1>Using a 4M model to detect Alzheimer's Disease</h1>
    <nav>
      <a href="#introduction">Introduction</a>
      <a href="#baselines">Baseline Models</a>
      <a href="#methodology">Methodology</a>
      <a href="#results">Results</a>
      <a href="#challenges">Challenges</a>
      <a href="#conclusion">Conclusion</a>
      <a href="#references">References</a>
    </nav>
  </header>

  <main>
    <article id="introduction" class="blog-post animate">
      <h2>Introduction</h2>
      <div class="tags">
        <span class="tag">Alzheimer's</span>
        <span class="tag">Multimodal Learning</span>
        
      </div>
      <p>
        Early diagnosis of neurological diseases like Alzheimer's improves treatment outcomes and reduces long-term impact.
        Since real-world diagnosis is multi-modal, we propose using such an approach for improved detection, inspired by recent
        cardiovascular disease research. We believe using a multi-input model like 4M will yield more accurate results in
        Alzheimer's prediction.
      </p>
    </article>
    
    <article id="baselines" class="blog-post animate">
      <h2>Baseline Models</h2>
      <div class="tags">
        <span class="tag">3D CNN</span>
        <span class="tag">SVM</span>
        <span class="tag">Benchmarks</span>
      </div>
      <p>
        The paper titled "Multimodal neuroimaging-based prediction of Parkinson's disease with mild cognitive impairment
        using machine learning technique" explores how machine learning can predict mild cognitive impairment
        in Parkinson's disease patients by integrating multiple data types.
      </p>
      <p>
        However, aside from the actual disease, its architecture also differs substantially from our experimental design. 
        While the paper proposes an SVM model that requires handcrafted, domain-specific feature extraction, we use a 
        modified Transformer encoder that relies solely on end-to-end modeling.
      </p>
      <div class="image-container">
        <figure>
          <img src="svm.png" alt="SVM Architecture" class="result-image">
          <figcaption>Figure 1: Previous approach using SVM for multimodal neuroimaging prediction</figcaption>
        </figure>
      </div>
      <p>
        Our baseline model is based on the work of Nhan and Nam, who proposed a 3D CNN architecture for Alzheimer's disease classification using structural MRI data with data augmentation techniques. Their model achieved an accuracy of 91.2 percent and an area under the curve (AUC) of 96.1 percent, demonstrating strong performance in distinguishing Alzheimer's patients from healthy controls. We use this as a reference point to evaluate the effectiveness of our proposed multi-modal Transformer-based approach.
      </p>
    </article>
    
    <article id="methodology" class="blog-post animate">
      <h2>Methodology and Data Used</h2>
      <div class="tags">
        <span class="tag">ADNI Database</span>
        <span class="tag">MRI</span>
        <span class="tag">PET Scans</span>
        <span class="tag">Transformer</span>
      </div>
      <p>
        We decided to use the ADNI database to train our model upon. The ADNI (Alzheimer's Disease Neuroimaging Initiative) database contains a plentitude of
        neurological brain scans from which we formed a cohort of specically MRI and PET scans tailored to our needs.
      </p>
      
      <details>
        <summary>Data Preprocessing Details</summary>
        <p>
          Before passing the imagery to our model, we have preprocessed it. Specifically, we have converted the dicom files into numpy array, normalized them and 
          rendered them more accessible and uniform for processing. We also removed all non-tissue parts from the scans (e.g. skull, etc..) and prioritized mid-brain
          regions abundant with grey matter.
        </p>
        <div class="preprocessing-images">
          <figure>
            <img src="mri_skull_removed.png" alt="MRI preprocessing steps" class="preprocessing-image">
            <figcaption>Figure 1: MRI preprocessing showing removal of non-tissue parts</figcaption>
          </figure>
          <figure>
            <img src="mr.png" alt="Multiple brain scan slices" class="preprocessing-image">
            <figcaption>Figure 2: Multiple slices of processed brain scans</figcaption>
          </figure>
        </div>
      </details>
      
      <h3>Model Architecture</h3>
      <p>
        Here is a brief overview of how our encoder-only Transformer is operating:
      </p>
      <ol>
        <li>All input images, genetic data, and clinical scores are tokenized and passed through a learnable embedding layer.</li>
        <li>To help the model distinguish between input types, we added modality-specific embeddings, because MRI tokens can carry different identity information than PET or APOE tokens.</li>
        <li>Our core architecture consists of stacked self-attention blocks, each containing:
          <ul>
            <li>A multi-head attention layer</li>
            <li>Feed-forward layers</li>
            <li>Residual connections</li>
            <li>A normalization layer</li>
          </ul>
        </li>
        <li>We allow flexibility in configuration (depth, attention head size, MLP ratio) so the model can be tuned for performance vs. computational cost.</li>
        <li>After being normalized and passing through two GELU layers, the pooled feature vector is converted into a category (chosen between normal, MCI and AD) by a softmax function.</li>
      </ol>
      
      <details class="code-details">
        <summary>View Model Implementation Code</summary>
        <div class="code-container">
          <div class="code-header">
            <span class="code-filename">adni_classifier.py</span>
            <button class="copy-btn" onclick="copyCode('code-adni')">Copy Code</button>
          </div>
          <pre><code id="code-adni" class="python">import torch.nn as nn
import torch
from modeling.transformer_layers import TransformerTrunk, LayerNorm

class AdniClassifier(nn.Module):
    def __init__(self, dim=512, depth=8, head_dim=64, mlp_ratio=4., num_classes=3):
        super().__init__()
        # Embeddings
        self.enc_tok_emb = nn.Embedding(  # vocab_size should cover max token+1, e.g. 101
            num_embeddings=101, embedding_dim=dim
        )
        self.enc_mod_emb = nn.Embedding(4, dim)   # MRI(0),PET(1),APOE(2),ADAS13(3)
        self.pos_emb     = nn.Parameter(torch.randn(1, 64000, dim))  

        # Encoder trunk
        self.encoder = TransformerTrunk(dim=dim, depth=depth, head_dim=head_dim, mlp_ratio=mlp_ratio)

        # Classification head
        self.classifier = nn.Sequential(
            LayerNorm(dim),
            nn.Linear(dim, dim//2),
            nn.GELU(),
            nn.Linear(dim//2, num_classes)
        )

    def forward(self, enc_tokens, enc_mods, enc_pos, enc_mask):
        # 1) Embedding
        x = self.enc_tok_emb(enc_tokens) + self.enc_mod_emb(enc_mods) + self.pos_emb[:, enc_pos, :]
        # 2) Encode
        x = self.encoder(x, mask=enc_mask)
        # 3) Mean-pool
        feat = x.mean(dim=1)
        # 4) Classify
        return self.classifier(feat)
</code></pre>
        </div>
      </details>
    </article>
    
    <article id="results" class="blog-post animate">
      <h2>Results</h2>
      <div class="tags">
        <span class="tag">Performance</span>
        <span class="tag">Training</span>
        <span class="tag">Evaluation</span>
      </div>
      <p>
        Our model has obtained the following results:
      </p>
      <div class="image-container">
        <figure>
          <img src="train_result.png" alt="Training Graph" class="result-image">
          <figcaption>Figure 3: Training performance showing loss decrease and accuracy improvement</figcaption>
        </figure>
        <figure>
          <img src="eval_result.png" alt="Eval Graph" class="result-image">
          <figcaption>Figure 4: Evaluation performance showing signs of overfitting</figcaption>
        </figure>
      </div>
      <p>
        In the training dataset, the loss steadily decreased and performance improved, eventually converging toward near-perfect accuracy. However, on the evaluation dataset, we observed an increase in loss and a decline in accuracy, indicating clear signs of overfittingâ€”likely due to the limited size of the available data. As a result, our model was unable to generalize effectivly and could not compete with our  baselines
      </p>
    </article>

    <article id="challenges" class="blog-post animate">
      <h2>Encountered Challenges</h2>
      <div class="tags">
        <span class="tag">Data Limitations</span>
        <span class="tag">Spatial Information</span>
        <span class="tag">Interpretability</span>
      </div>
      <p>
        Throughout the implementation and evaluation of our multimodal transformer-based approach, we encountered several significant challenges that impacted the overall performance and generalizability of our model. These obstacles provide valuable insights for future research in this domain.
      </p>
      <h3>Limited Dataset Size</h3>
      <p>
        The primary constraint we faced was the limited availability of high-quality, annotated neuroimaging data. Having selected a specific cohort from the ADNI database to maintain demographic and clinical homogeneity, we were restricted to working with only several hundred samples. This limitation substantially hindered our model's ability to learn robust representations across multiple modalities, as transformer architectures typically require large-scale datasets to achieve optimal performance. The scarcity of data led to overfitting issues that were evident in our evaluation metrics, despite implementing regularization techniques and data augmentation strategies.
      </p>
      <h3>Spatial Information Representation</h3>
      <div class="highlight">
        <p><i class="fas fa-exclamation-triangle"></i> A critical technical challenge we identified was the inadequacy of 2D token representations in capturing the complex spatial relationships within neuroimaging data. The flattened 2D tokens failed to preserve the crucial three-dimensional structural information that is essential for accurate Alzheimer's disease classification. We hypothesize that implementing 3D customized Vector-Quantized Variational Autoencoder (VQ-VAE) tokens would significantly enhance the model's ability to extract and utilize spatial features, potentially leading to substantial improvements in diagnostic accuracy.</p>
      </div>
      <h3>Model Interpretability</h3>
      <p>
        The interpretability of transformer models presents a substantial challenge compared to more classical machine learning approaches. While our model demonstrated the ability to process multimodal inputs, the attention mechanisms and learned representations remain largely opaque "black boxes." This lack of interpretability poses significant barriers to clinical adoption, as healthcare professionals require transparent reasoning behind diagnostic suggestions. Future work should focus on developing visualization techniques and attribution methods specifically tailored to transformer-based medical image analysis to bridge this gap between performance and explainability.
      </p>
     
      <button class="btn" onclick="toggleSection('conclusion')">See Our Conclusions</button>
    </article>

    <article id="conclusion" class="blog-post animate">
      <h2>Conclusion</h2>
      <p>
        In conclusion, our modified Transformer encoder aimed to integrate MRI, PET, genetic, and clinical data within a unified attention-based framework, offering a flexible alternative to traditional convolutional approaches. However, its performance was lower than expected, likely due to the limited size of the available dataset, which constrained the model's ability to learn meaningful multi-modal representations. These findings underscore the importance of larger, high-quality datasets for developing effective attention-based diagnostic models and suggest that, with more data, this approach may still hold promise for complex biomedical tasks like Alzheimer's disease diagnosis.
      </p>
    </article>
    
    <article id="references" class="blog-post animate">
      <h2>References</h2>
      <ol class="references-list">
        <li>
          Y. Zhu, F. Wang, P. Ning, Y. Zh et al., "Multimodal neuroimaging-based prediction of parkinson's disease with
          mild cognitive impairment using machine learning technique," NPJ Parkinson's Disease, 2024, published in partnership with
          the Parkinson's Foundation.
        </li>
        <li>
          V. T. Nhan and H. B. Nam, "3D Brain MRI Classification for Alzheimer's Diagnosis Using CNN with Data Augmentation," 
          in Proceedings of the International Conference on Artificial Intelligence and Data Science (ICAIDS), 
          Ho Chi Minh City, Vietnam: IEEE, 2024.
        </li>
      </ol>
    </article>
  </main>
  <footer>
    <p>&copy; 2025 semester project</p>
    <p>by Heeyoung Lee, Finn Mac Namara and Fedor Mitirev</p>
    <div class="social-links">
      <a href="#" class="social-link"><i class="fab fa-github"></i></a>
      <a href="#" class="social-link"><i class="fab fa-linkedin"></i></a>
      <a href="#" class="social-link"><i class="fab fa-twitter"></i></a>
    </div>
  </footer>

  <script>
    // Add smooth scrolling for navigation links
    document.querySelectorAll('nav a').forEach(anchor => {
      anchor.addEventListener('click', function(e) {
        e.preventDefault();
        const targetId = this.getAttribute('href');
        const targetElement = document.querySelector(targetId);
        window.scrollTo({
          top: targetElement.offsetTop - 80,
          behavior: 'smooth'
        });
      });
    });

    // Function to toggle sections
    function toggleSection(sectionId) {
      const section = document.getElementById(sectionId);
      section.scrollIntoView({ behavior: 'smooth', block: 'start' });
    }

    // Add animation when elements come into view
    const observer = new IntersectionObserver((entries) => {
      entries.forEach(entry => {
        if (entry.isIntersecting) {
          entry.target.classList.add('visible');
        }
      });
    }, { threshold: 0.1 });

    // Observe all animate elements
    document.querySelectorAll('.animate').forEach(element => {
      observer.observe(element);
    });
    
    // Function to copy code to clipboard
    function copyCode(elementId) {
      const codeElement = document.getElementById(elementId);
      const textArea = document.createElement('textarea');
      textArea.value = codeElement.textContent;
      document.body.appendChild(textArea);
      textArea.select();
      document.execCommand('copy');
      document.body.removeChild(textArea);
      
      // Show feedback
      const copyBtn = event.target;
      const originalText = copyBtn.textContent;
      copyBtn.textContent = 'Copied!';
      setTimeout(() => {
        copyBtn.textContent = originalText;
      }, 2000);
    }
  </script>
</body>
</html>
